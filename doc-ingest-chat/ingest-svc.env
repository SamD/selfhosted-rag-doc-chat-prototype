# Document Ingestion System Environment Variables
# Copy this file to .env and modify as needed
#
# IMPORTANT: When using Docker Compose, you must set the following host-side variables in your environment or .env file:
#   HOST_DATA_DIR, HOST_OCR_DEBUG_DIR, HOST_E5_MODEL_DIR, HOST_LLAMA_MODEL_FILE
# These must be set to the correct absolute paths on your host system to avoid container failures.
# The right side of each mount (container path) is set by the variables below and must match what your code expects.

DEBUG_IMAGE_DIR=/tmp/ocr_debug

FAILED_FILES=${INGEST_FOLDER}/failed_files.txt
INGESTED_FILE=${INGEST_FOLDER}/ingested_files.txt
TRACK_FILE=${INGEST_FOLDER}/ingested_files.txt
PARQUET_FILE=${INGEST_FOLDER}/chunks.parquet
DUCKDB_FILE=${INGEST_FOLDER}/chunks.duckdb

# Redis Configuration (use service name and internal port for Docker Compose)
REDIS_HOST=redis
REDIS_PORT=6380
REDIS_OCR_JOB_QUEUE=ocr_processing_job
REDIS_INGEST_QUEUE=chunk_ingest_queue

# Queue Configuration
QUEUE_NAMES=chunk_ingest_queue:0,chunk_ingest_queue:1,chunk_ingest_queue:2,chunk_ingest_queue:3

# ChromaDB Configuration (use service name and internal port for Docker Compose)
CHROMA_HOST=chromadb
CHROMA_PORT=8000
CHROMA_COLLECTION=chroma_collection

# LLM and Chat Configuration
USE_OLLAMA=0
# OLLAMA_MODEL=openchat
OLLAMA_MODEL=NeuralNet/openchat-3.6
# not used
# OLLAMA_URL=http://localhost:11434


# OCR Configuration
# DEBUG_IMAGE_DIR: Directory inside the container for OCR debug output. Must match the right side of the OCR debug volume mount.
MAX_OCR_DIM=3000

# Media Processing Configuration
SUPPORTED_MEDIA_EXT=.mp3,.wav,.m4a,.aac,.flac,.mp4,.mov,.mkv

# Whisper Configuration
DEVICE=cuda
MEDIA_BATCH_SIZE=8
COMPUTE_TYPE=float16

# File Processing Configuration
CHUNK_TIMEOUT=300
MAX_CHUNKS=20000

#The maximum number of chunks (documents) passed to db.add_texts() in one call
# Controls the batch size for embeddings
MAX_CHROMA_BATCH_SIZE=75

# The upper bound on total token count across the batch, before flushing to Chroma
# These are the max elements pushed to redis from the producer per step
MAX_CHROMA_BATCH_SIZE_LIMIT=5461

MAX_TOKENS=512

# Queue Management
MAX_QUEUE_LENGTH=25
POLL_INTERVAL=0.5
WAIT_WARN_THRESHOLD=10


# START GPU BASED LIKELY MORE ACCURATE BASED ON TOKENS + TO_K
# Applies to vectorstore
# Retrieve the top k most relevant chunks (based on vector similarity) from the database for a given query.
#RETRIEVER_TOP_K=20
#
#LLAMA_N_CTX=32768
#LLAMA_N_GPU_LAYERS=35
#LLAMA_N_THREADS=12
#LLAMA_N_BATCH=512
#LLAMA_F16_KV=True
#LLAMA_TEMPERATURE=0.3
#LLAMA_TOP_K=40
#LLAMA_TOP_P=0.85
#LLAMA_REPEAT_PENALTY=1.2
## Number of new tokens the LLM is allowed to generate
## more tokens longer / multi-section output but longer to produce
#LLAMA_MAX_TOKENS=4096
#LLAMA_CHAT_FORMAT=chatml
#LLAMA_VERBOSE=False
#LLAMA_SEED=42
# END GPU BASED LIKELY


# START FASTER INGEST / QUERY DEMO
# Less accurate, smaller top-k results
RETRIEVER_TOP_K=5
LLAMA_N_GPU_LAYERS=35
LLAMA_N_BATCH=128
LLAMA_MAX_TOKENS=256
LLAMA_TOP_K=20
LLAMA_TEMPERATURE=0.2
# END FASTER INGEST / QUERY DEMO

# Metrics Configuration
# Enable or disable performance metrics collection (true/false)
METRICS_ENABLED=true
# File path for metrics output (JSONL format)
METRICS_LOG_FILE=${INGEST_FOLDER}/metrics.jsonl
# Also log metrics to stdout (true/false)
METRICS_LOG_TO_STDOUT=true
